{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, ContentFormat, AnalyzeResult\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from config import AZURE_STORAGE_CONNECTION_STRING, AZURE_DOC_INTEL_ENDPOINT, AZURE_DOC_INTEL_KEY\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from config import AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "prebuilt_layout_model = \"prebuilt-layout\"\n",
    "\n",
    "# Constants\n",
    "AZURE_OPENAI_TEMP = 0\n",
    "AZURE_OPENAI_MAX_TOKENS = 2500\n",
    "RESULTS_DIR = \"results\"\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(azure_endpoint=AZURE_OPENAI_ENDPOINT, api_key=AZURE_OPENAI_KEY, api_version=\"2024-08-01-preview\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(local_file_path, prebuilt_model=\"prebuilt-layout\"):\n",
    "    document_intelligence_client = DocumentIntelligenceClient(endpoint=AZURE_DOC_INTEL_ENDPOINT, credential=AzureKeyCredential(AZURE_DOC_INTEL_KEY))\n",
    "    try:\n",
    "        with open(local_file_path, 'rb') as f:\n",
    "            poller = document_intelligence_client.begin_analyze_document(\n",
    "                prebuilt_model, analyze_request=f, content_type=\"application/octet-stream\"\n",
    "            )\n",
    "            result = poller.result()\n",
    "    except Exception as e:\n",
    "        return f\"Failed to analyze document: {e}\"\n",
    "\n",
    "    markdown_lines = []\n",
    "    if result.styles is None:\n",
    "        # Handle the None case, e.g., log an error or return an empty list\n",
    "        print(\"Result.styles is None\")\n",
    "        #return markdown_lines\n",
    "    else:\n",
    "        for idx, style in enumerate(result.styles):\n",
    "            print(\"Result.styles is not None\")\n",
    "            #markdown_lines.append(\n",
    "            #    f\"Document contains {'handwritten' if style.is_handwritten else 'no handwritten'} content\"\n",
    "            #)\n",
    "\n",
    "    for page in result.pages:\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            markdown_lines.append(\n",
    "                f\"...Line # {line_idx}: '{line.content}'\"\n",
    "            )\n",
    "        #if page.selection_marks is not None:\n",
    "        #    for selection_mark in page.selection_marks:\n",
    "        #        markdown_lines.append(\n",
    "        #            f\"...Selection mark is '{selection_mark.state}' and has a confidence of {selection_mark.confidence}\"\n",
    "        #        )\n",
    "    if result.tables is not None:\n",
    "        for table_idx, table in enumerate(result.tables):\n",
    "            markdown_lines.append(\n",
    "                f\"Table # {table_idx} has {table.row_count} rows and {table.column_count} columns\"\n",
    "            )\n",
    "\n",
    "            for cell in table.cells:\n",
    "                markdown_lines.append(\n",
    "                    f\"...Cell[{cell.row_index}][{cell.column_index}] has content '{cell.content}'\"\n",
    "                )\n",
    "\n",
    "    markdown_lines.append(\"----------------------------------------\")\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "    #return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define the data folder path\n",
    "data_folder = os.path.join(current_dir, 'data', 'new_data')\n",
    "\n",
    "# List all files in the data folder\n",
    "files = os.listdir(data_folder)\n",
    "\n",
    "# Filter out image files (assuming jpg and png formats)\n",
    "image_files = [f for f in files if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Output the array of image files\n",
    "print(f\"Image files: {image_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = []\n",
    "\n",
    "for image_file in image_files:\n",
    "    check_result = analyze_document(os.path.join(data_folder, image_file), prebuilt_layout_model)\n",
    "    check_results.append({\n",
    "        \"image_url\": os.path.join(data_folder, image_file),\n",
    "        \"result\": check_result\n",
    "    })\n",
    "\n",
    "    # save the result for each check to a JSON file in the results directory (create if it doesn't exist). Save as a subdirectory 'ocr_results'\n",
    "    results_dir = os.path.join(current_dir, RESULTS_DIR, \"ocr_results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    result_file_path = os.path.join(results_dir, f\"{os.path.splitext(image_file)[0]}_ocr_result.json\")\n",
    "    with open(result_file_path, 'w') as result_file:\n",
    "        result_file.write(check_result)  # Write the JSON result to the file\n",
    "    \n",
    "    print(f\"Results for {image_file}:\\n{check_result}\\n\")\n",
    "\n",
    "# Output the array of check results\n",
    "print(check_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logprobs_handler_custom import LogprobsHandler\n",
    "\n",
    "# Initialize the LogprobsHandler\n",
    "logprobs_handler = LogprobsHandler()\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encode image to base64.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def extract_key_fields_with_gpt(image_path: str, doc_intel_result: str):\n",
    "    \"\"\"Extract key marked up and filled out fields and checkboxes from an image of a check.\"\"\"\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    # First prompt to extract key fields\n",
    "    response1 = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant responsible for extracting key fields from the image of a check, with the assistance of an OCR tool. Return the output in the specified JSON format.\"},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \n",
    "                 f\"\"\"Extract the following fields from the image of the check: \\\n",
    "                 - CheckDate: Date the check was written. Date should be returned in the format MM/DD/YYYY. \\\n",
    "                 - Payee: Name of the person or entity the check is made out to \\\n",
    "                 - Amount: Amount check was paid out for, return as a 2-decimal number (verify using both numberAmount and wordAmount) \\\n",
    "                 - MICR (4 digits): Check number, usually at the bottom of the check \\\n",
    "                 \n",
    "                 In order to extract the fields reliably, follow the following steps: \\\n",
    "                 1. Understand the OCR output and extract the key fields from the OCR markdown output. \\\n",
    "                 2. Verify the extracted OCR fields against the image to ensure accuracy. \\\n",
    "                    Note: Sometimes the image contains additional detail alongside the check, so please ensure you only look at the check. \\\n",
    "                 3. Fix any inconsistencies in the field values, and double check with the OCR output. \\\n",
    "                 4. Return the extracted fields in the specified JSON format. \\\n",
    "                 \n",
    "                 OCR Markdown Output: \\\n",
    "                 {doc_intel_result} \\\n",
    "                                  \n",
    "                 Output JSON Schema: \\\n",
    "                 file_name: Name of the file \\\n",
    "                 fields: array of objects containing the following fields: \\\n",
    "                    - field_name: Name of the field \\\n",
    "                    - field_value: Value of the field \\\n",
    "                \n",
    "                 Return result below: \\\n",
    "                 -------------------------------------------\"\"\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "            ]}\n",
    "        ],\n",
    "        temperature=AZURE_OPENAI_TEMP,\n",
    "        max_tokens=AZURE_OPENAI_MAX_TOKENS,\n",
    "        logprobs=True\n",
    "    )\n",
    "    \n",
    "    gpt_extracted_fields = response1.choices[0].message.content\n",
    "    \n",
    "    # Extract the log probabilities from the response\n",
    "    response_logprobs = response1.choices[0].logprobs.content if hasattr(response1.choices[0], 'logprobs') else []\n",
    "\n",
    "    # Format the logprobs\n",
    "    logprobs_formatted = logprobs_handler.format_logprobs(response_logprobs)\n",
    "    #print(logprobs_formatted)\n",
    "\n",
    "    paired_probs = logprobs_handler.calculate_words_probas(logprobs_formatted)\n",
    "    gpt_extracted_fields = logprobs_handler.calculate_confidence_scores(paired_probs)\n",
    "    \n",
    "    return gpt_extracted_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the check results and run the results through the extract_key_fields_with_gpt function\n",
    "for check in check_results:\n",
    "    image_path = check[\"image_url\"]\n",
    "    doc_intel_result = check[\"result\"]\n",
    "    gpt_result = extract_key_fields_with_gpt(image_path, doc_intel_result)\n",
    "    print(f\"gpt_result for {image_path}: {gpt_result}\")\n",
    "    # add filename to the JSON result\n",
    "    gpt_result_dict = gpt_result\n",
    "    gpt_result_dict[\"file_name\"] = os.path.basename(image_path)\n",
    "    gpt_result = json.dumps(gpt_result_dict)\n",
    "    \n",
    "    # Save the result for each check to a JSON file in the results directory (create if it doesn't exist). Save as a subdirectory 'gpt_results'\n",
    "    results_dir = os.path.join(current_dir, RESULTS_DIR, 'gpt_results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    result_file_path = os.path.join(results_dir, f\"{os.path.splitext(os.path.basename(image_path))[0]}_gpt_result.json\")\n",
    "    \n",
    "    with open(result_file_path, 'w') as result_file:\n",
    "        result_file.write(gpt_result)  # Write the JSON result to the file\n",
    "    \n",
    "    print(f\"GPT result for {image_path}:\\n{gpt_result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define the results directory\n",
    "results_dir = os.path.join(current_dir, RESULTS_DIR)\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Function to extract data from JSON files\n",
    "def extract_data_from_json(file_path, result_type):\n",
    "    with open(file_path, 'r') as file:\n",
    "        result = json.load(file)\n",
    "        fields = {field[\"field_name\"]: field[\"field_value\"] for field in result[\"fields\"]}\n",
    "        confidences = {f\"{field['field_name']}_confidence\": field.get(\"field_confidence\", None) for field in result[\"fields\"]}\n",
    "        fields.update(confidences)\n",
    "        fields[\"file_name\"] = result[\"file_name\"]\n",
    "        fields[\"result_type\"] = result_type\n",
    "        return fields\n",
    "\n",
    "# Iterate through the GPT results\n",
    "gpt_results_dir = os.path.join(results_dir, \"gpt_results\")\n",
    "for json_file in os.listdir(gpt_results_dir):\n",
    "    if json_file.endswith(\".json\"):\n",
    "        file_path = os.path.join(gpt_results_dir, json_file)\n",
    "        data.append(extract_data_from_json(file_path, \"gpt_result\"))\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the desired columns\n",
    "columns = [\"file_name\", \"CheckDate\", \"CheckDate_confidence\", \"Payee\", \"Payee_confidence\", \"Amount\", \"Amount_confidence\", \"MICR\", \"MICR_confidence\", \"result_type\"]\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df = df[columns]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = os.path.join(results_dir, \"results_summary.csv\")\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file generated at: {csv_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp_agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
